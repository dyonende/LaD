{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dyon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "import json\n",
    "import pandas as pd\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import stanza\n",
    "import string\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "### helper functions ###\n",
    "\n",
    "# convert to DE and NL files to dataframe\n",
    "def load_json_files(path, lang):\n",
    "    _path = path + lang\n",
    "    files = os.listdir(_path)\n",
    "    data = list()\n",
    "    for filename in files:\n",
    "        filepath = os.path.join(_path, filename)\n",
    "        with open(filepath) as file:\n",
    "            data.append(json.load(file))\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# MT does not posses any lemmatization and there for cannot be preprocessed\n",
    "def preprocess_de(article):\n",
    "    stopwords = nltk.corpus.stopwords.words('german')\n",
    "    processed_article = data['de']['nlp'].process(article)\n",
    "    all_lemmas = []\n",
    "    for s in processed_article.sentences: \n",
    "        clean_lemmas = list()\n",
    "        for word in s.words:\n",
    "            if word.text.lower() not in stopwords:\n",
    "                lemma = word.lemma.lower()\n",
    "                if lemma not in string.punctuation:\n",
    "                    clean_lemmas.append(lemma)\n",
    "        all_lemmas.extend(clean_lemmas)\n",
    "    return all_lemmas\n",
    "\n",
    "def preprocess_nl(article):\n",
    "    stopwords = nltk.corpus.stopwords.words('dutch')\n",
    "    processed_article = data['nl'].process(article)\n",
    "    all_lemmas = []\n",
    "    for s in processed_article.sentences: \n",
    "        clean_lemmas = list()\n",
    "        for word in s.words:\n",
    "            if word.text.lower() not in stopwords:\n",
    "                lemma = word.lemma.lower()\n",
    "                if lemma not in string.punctuation:\n",
    "                    clean_lemmas.append(lemma)\n",
    "        all_lemmas.extend(clean_lemmas)\n",
    "    return all_lemmas\n",
    "\n",
    "\n",
    "def train_custom_model(dataset):\n",
    "    articles = dataset['df']['text']\n",
    "    tokenizer = dataset['nlp']\n",
    "    tokenized = []\n",
    "    for article in articles:\n",
    "        for sent in tokenizer(article).sentences:\n",
    "            tokenized.append([tok.text.lower() for tok in sent.tokens])\n",
    "\n",
    "    # Train a Word2Vec model, the min_count parameter indicates the minimum frequency of each word in the corpus\n",
    "    mymodel = Word2Vec(tokenized, min_count=2)\n",
    "\n",
    "    # summarise vocabulary\n",
    "    words = list(mymodel.wv.vocab)\n",
    "\n",
    "    return mymodel\n",
    "\n",
    "\n",
    "def visualize_word_vectors(dataset, lang, model, vocab):\n",
    "    # Apply dimensionality reduction with PCA or T-SNE\n",
    "    high_dimensional = model[vocab]\n",
    "    reduction_technique = TSNE(n_components=2)\n",
    "\n",
    "    print(\"Calculate dimensionality reduction\")\n",
    "    two_dimensional = reduction_technique.fit_transform(high_dimensional)\n",
    "    print(\"Done\")\n",
    "\n",
    "    # Get the indices in the vocabulary for selected terms\n",
    "    terms = dataset['wv_terms']\n",
    "    vocab_list = list(vocab.keys())\n",
    "    term_indices = [vocab_list.index(term) for term in terms]\n",
    "\n",
    "    print(term_indices)\n",
    "\n",
    "    # Plot the two-dimensional vectors for the selected terms\n",
    "    x_values = [two_dimensional[index, 0] for index in term_indices]\n",
    "    y_values = [two_dimensional[index, 1] for index in term_indices]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (15, 10))\n",
    "\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(terms)))\n",
    "    for x, y, c in zip(x_values, y_values, colors):\n",
    "        ax.plot(x, y, 'o', markersize=12, color=c)\n",
    "\n",
    "    # Add title and description\n",
    "    ax.set_title(lang+' terms')\n",
    "\n",
    "    # Annotate the terms in the plot\n",
    "    for i, word in enumerate(terms):\n",
    "        plt.annotate(word, xy=(x_values[i], y_values[i]), fontsize = 16)\n",
    "\n",
    "    # legend\n",
    "    plt.legend(dataset['wv_terms_translations'])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_top_tfidf_features(row, terms, top_n=25):\n",
    "    top_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_features = [terms[i] for i in top_ids]\n",
    "    return top_features\n",
    "\n",
    "\n",
    "def wordcloud_cluster_byIds(clusterId, clusters, keywords, lang):\n",
    "    words = []\n",
    "    for i in range(0, len(clusters)):\n",
    "        if clusters[i] == clusterId:\n",
    "            for word in keywords[i]:\n",
    "                words.append(word)\n",
    "    print(words)\n",
    "    # Generate a word cloud based on the frequency of the terms in the cluster\n",
    "    wordcloud = WordCloud(max_font_size=40, relative_scaling=.8).generate(' '.join(words))\n",
    "   \n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(lang+str(clusterId)+\".png\")\n",
    "\n",
    "\n",
    "def cluster_wordcloud(dataset, lang):\n",
    "    news_content = dataset['df']\n",
    "\n",
    "    # We filter out empty articles\n",
    "    news_content = news_content[news_content[\"text\"].str.len() >0 ]\n",
    "    articles = news_content[\"text\"]\n",
    "\n",
    "    # You can play around with the ngram range\n",
    "    if k == 'de':\n",
    "        vectorizer = TfidfVectorizer(use_idf=True, tokenizer=preprocess_de)\n",
    "    elif k == 'nl':\n",
    "        vectorizer = TfidfVectorizer(use_idf=True, tokenizer=preprocess_nl)\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(use_idf=True)\n",
    "        \n",
    "    tf_idf = vectorizer.fit_transform(articles)\n",
    "    all_terms = vectorizer.get_feature_names()\n",
    "    # print(all_terms[0:30])\n",
    "\n",
    "    # extract the keywords\n",
    "    num_keywords = 10\n",
    "\n",
    "\n",
    "    keywords = []\n",
    "    for i in range(0, tf_idf.shape[0]):\n",
    "        row = np.squeeze(tf_idf[i].toarray())\n",
    "        top_terms_for_article= get_top_tfidf_features(row, all_terms, top_n=num_keywords)\n",
    "        # print(\"Keywords for article \" + str(i))\n",
    "        # print(top_terms_for_article)\n",
    "        keywords.append(top_terms_for_article)\n",
    "\n",
    "\n",
    "    # document representations\n",
    "    all_doc_representations = []\n",
    "\n",
    "    for doc_keywords in keywords:\n",
    "        doc_representation =[]\n",
    "        for keyword in doc_keywords:\n",
    "            keyword_with_capital = keyword[0].upper() + keyword[1:]\n",
    "            if keyword in dataset['fasttext_model'].vocab:            \n",
    "                word_representation = dataset['fasttext_model'].get_vector(keyword)\n",
    "                doc_representation.append(word_representation)\n",
    "            elif keyword_with_capital in dataset['fasttext_model'].vocab:\n",
    "                word_representation = dataset['fasttext_model'].get_vector(keyword_with_capital)\n",
    "                doc_representation.append(word_representation)\n",
    "            else:\n",
    "                # We simply ignore unknown words\n",
    "                print(keyword)\n",
    "            \n",
    "            \n",
    "        # Take the mean over the keywords\n",
    "        mean_keywords = np.mean(doc_representation, axis=0)\n",
    "        all_doc_representations.append(mean_keywords)\n",
    "\n",
    "    # Number of clusters\n",
    "    from sklearn.cluster import KMeans\n",
    "    num_clusters = 4\n",
    "    km = KMeans(n_clusters=num_clusters)\n",
    "    km.fit(all_doc_representations)\n",
    "\n",
    "    # Output the clusters\n",
    "    clusters = km.labels_.tolist()\n",
    "    clustered_articles ={'link': news_content[\"link\"],'website': news_content[\"website\"],'text': news_content[\"text\"], 'cluster': clusters}\n",
    "    overview = pd.DataFrame(clustered_articles, columns = ['link', 'website', 'text', 'Cluster'])\n",
    "\n",
    "\n",
    "    # display wordcloud for cluster 3\n",
    "    wordcloud_cluster_byIds(3, clusters, keywords, lang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading german model ...done\n",
      "loading dutch model ...done\n",
      "loading maltese model ...done\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/\"\n",
    "\n",
    "# load data\n",
    "de_df = load_json_files(path, 'de')\n",
    "nl_df = load_json_files(path, 'nl')\n",
    "mt_df = pd.read_csv(path+'mt/articles_mt.tsv', sep='\\t', header=0)\n",
    "\n",
    "data = {\n",
    "    'de': {\n",
    "        'df': de_df,\n",
    "        'wv_terms': [\"gesetz\", \"vergewaltigung\", \"schwangerschaft\", \"frauen\", \"abtreibung\", \"abtreibungen\", \"leben\", \"polen\", \"mehr\", \"schwangerschaftsabbruch\", \"sagt\", \"sei\", \"kirche\"],\n",
    "        'wv_terms_translations': [\"gesetz (law)\", \"vergewaltigung (rape)\", \"schwangerschaft (pregnancy)\", \"frauen (women)\", \"abtreibung (abortion)\", \"abtreibungen (abortions)\", \"leben (life)\", \"polen (poland)\", \"mehr (more)\", \"schwangerschaftsabbruch (abortion)\", \"sagt (says)\", \"sei (would be)\", \"kirche (church)\"]\n",
    "        },\n",
    "    'nl': {\n",
    "        'df': nl_df,\n",
    "        'wv_terms': [\"wet\", \"zwangerschap\", \"abortus\", \"vrouwen\", \"we\", \"jaar\", \"nederland\", \"mensen\", \"vrouw\", \"wel\", \"waar\", \"weken\"],\n",
    "        'wv_terms_translations': [\"wet (law)\", \"zwangerschap (pregnancy)\", \"abortus (abortion)\", \"vrouwen (women)\", \"we (we)\", \"jaar (year)\", \"Nederland (The Netherlands)\", \"mensen (people)\", \"vrouw (woman)\", \"wel (well)\", \"waar (true / where)\", \"weken (weeks)\"]\n",
    "        }, \n",
    "    'mt': {\n",
    "        'df': mt_df,\n",
    "        'wv_terms': [\"abort\", \"stupru\", \"liġi\", \"dritt\", \"tqala\", \"mewt\", \"gvern\", \"nisa\"],\n",
    "        'wv_terms_translations': [\"abort (abortion)\", \"stupru (rape)\", \"liġi (law)\", \"dritt (right)\", \"tqala (pregnancy)\", \"mewt (death)\", \"gvern (government)\", \"nisa (women)\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# load models\n",
    "\n",
    "print(\"loading german model ...\", end='')\n",
    "data['de']['fasttext_model'] = KeyedVectors.load_word2vec_format(\"./models/german.model\", binary=True)\n",
    "print(\"done\")\n",
    "print(\"loading dutch model ...\", end='')\n",
    "data['nl']['fasttext_model'] = KeyedVectors.load_word2vec_format(\"./models/model.bin\", binary=True)\n",
    "print(\"done\")\n",
    "print(\"loading maltese model ...\", end='')\n",
    "data['mt']['fasttext_model'] = KeyedVectors.load_word2vec_format(\"./models/cc.mt.300.vec\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 17.5MB/s]                    \n",
      "2020-12-10 17:27:08 INFO: Downloading default packages for language: de (German)...\n",
      "2020-12-10 17:27:09 INFO: File exists: /home/dyon/stanza_resources/de/default.zip.\n",
      "2020-12-10 17:27:18 INFO: Finished downloading models and saved to /home/dyon/stanza_resources.\n",
      "2020-12-10 17:27:18 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "=======================\n",
      "\n",
      "2020-12-10 17:27:18 INFO: Use device: cpu\n",
      "2020-12-10 17:27:18 INFO: Loading: tokenize\n",
      "2020-12-10 17:27:18 INFO: Loading: pos\n",
      "2020-12-10 17:27:18 INFO: Loading: lemma\n",
      "2020-12-10 17:27:19 INFO: Done loading processors!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/home/dyon/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'df'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-15080cb8d8d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# describe (basic statistics)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'df'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dyon/.local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2902\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2903\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2904\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dyon/.local/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2899\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'df'"
     ]
    }
   ],
   "source": [
    "for k, v in data.items():\n",
    "    # Prepare the nlp pipeline for all languages\n",
    "    stanza.download(k)\n",
    "    v['nlp'] = stanza.Pipeline(k, processors='tokenize,pos,lemma')\n",
    "\n",
    "    # fill missing data with an empty string\n",
    "    v = v['df'].fillna('')\n",
    "    \n",
    "    # describe (basic statistics)\n",
    "    print(k, v['df'].describe())\n",
    "\n",
    "\n",
    "    from sklearn.decomposition import PCA \n",
    "    from sklearn.manifold import TSNE\n",
    "    # train custom model and visualize word vectors with that model\n",
    "    model = train_custom_model(v)\n",
    "    visualize_word_vectors(v, k, model, model.wv.vocab)\n",
    "\n",
    "    # display a cluster wordcloud for each language\n",
    "    cluster_wordcloud(v, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
